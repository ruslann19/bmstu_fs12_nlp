{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18135bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b1a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import clean_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.bert_regressor import BertRegressor, BertRegressorConfig\n",
    "from src import compute_metrics\n",
    "from src import KeepBestModelCallback\n",
    "from src.predict_from_dataset import predict_from_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953a3e6",
   "metadata": {},
   "source": [
    "# 1. Подготовка данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9be906f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка датасета...\n",
      "Очистка текстов...\n",
      "Разделение на train/val...\n",
      "Готово\n"
     ]
    }
   ],
   "source": [
    "nrows = None\n",
    "\n",
    "# Загрузка данных из CSV\n",
    "print(\"Загрузка датасета...\")\n",
    "dataset_df = pd.read_csv(\n",
    "    filepath_or_buffer=\"./data/train.csv\",\n",
    "    nrows=nrows,\n",
    ")\n",
    "\n",
    "dataset_df = dataset_df[[\"text\", \"rating\"]]\n",
    "dataset_df = dataset_df.rename(columns={\"rating\": \"labels\"})\n",
    "\n",
    "# Выкинуть оценки, равные 0\n",
    "dataset_df = dataset_df[dataset_df[\"labels\"] != 0]\n",
    "\n",
    "if nrows is None:\n",
    "    # Оставить только 50_000 оценок, равных 5, для балансировки распределения оценок\n",
    "    dataset_df_5 = dataset_df[dataset_df[\"labels\"] == 5].sample(\n",
    "        n=50_000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    dataset_df_other = dataset_df[dataset_df[\"labels\"] != 5]\n",
    "    dataset_df = pd.concat([dataset_df_5, dataset_df_other]).reset_index(drop=True)\n",
    "\n",
    "print(\"Очистка текстов...\")\n",
    "dataset_df[\"text\"] = dataset_df[\"text\"].map(clean_text)\n",
    "\n",
    "\n",
    "print(\"Разделение на train/val...\")\n",
    "if nrows is None:\n",
    "    train_df, val_df = train_test_split(\n",
    "        dataset_df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        # Во всём датасете найдутся объекты, чтобы разделить их на train/val\n",
    "        stratify=dataset_df[\"labels\"],\n",
    "    )\n",
    "else:\n",
    "    train_df, val_df = train_test_split(\n",
    "        dataset_df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "train_df = train_df.astype({\"labels\": \"float\"})\n",
    "val_df = val_df.astype({\"labels\": \"float\"})\n",
    "\n",
    "train_data = Dataset.from_pandas(train_df)\n",
    "val_data = Dataset.from_pandas(val_df)\n",
    "\n",
    "print(\"Готово\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c35bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./data/test.csv\", nrows=None)\n",
    "test_df = test_df[[\"review_text\"]]\n",
    "test_df = test_df.rename(columns={\"review_text\": \"text\"})\n",
    "test_df[\"text\"] = test_df[\"text\"].map(clean_text)\n",
    "\n",
    "test_data = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6395b89",
   "metadata": {},
   "source": [
    "# 2. Загрузка токенизатора\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59e4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_name = \"seara/rubert-tiny2-russian-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153ca670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc9485e12884dd18c43d33449349633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127428 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1603086b709240e49de61e168119a5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0c14809025421cb980fc8d6a105723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5069 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=hf_model_name)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "val_data = val_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83260449",
   "metadata": {},
   "source": [
    "# Получение эмбэддингов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f7a34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_embeddings_from_dataset(\n",
    "    model_path: str,\n",
    "    dataset: Dataset,\n",
    ") -> np.ndarray:\n",
    "    model = BertRegressor.from_pretrained(model_path)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Устанавливаем формат\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    # Создаем DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            # Переносим на устройство\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "            all_embeddings.append(embeddings)\n",
    "\n",
    "    # Объединяем предсказания\n",
    "    predictions = np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00226b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c023924a4d349a8aaef76cd9b686275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"./data/embeddings/\", exist_ok=True)\n",
    "\n",
    "datasets = {\n",
    "    \"train\": train_data,\n",
    "    \"val\": val_data,\n",
    "    \"test\": test_data,\n",
    "}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    embeddings = get_embeddings_from_dataset(\"./checkpoints/rubert_linear/\", dataset)\n",
    "    embeddings_df = pd.DataFrame(embeddings)\n",
    "    embeddings_df.to_csv(\n",
    "        f\"./data/embeddings/{name}_embeddings.csv\",\n",
    "        index=False,\n",
    "        header=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7b483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
